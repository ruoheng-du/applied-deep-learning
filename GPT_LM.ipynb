{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9abce638",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9abce638",
        "outputId": "76d59ee6-899a-4f96-bf07-8874bf014469"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7988b54d2fb0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters.\n",
        "# I suggest you start with very small values, unless you have a strong PC or are running on the cluster\n",
        "batch_size = 64 # How many independent sequences will we process in parallel?\n",
        "block_size = 128 # What is the maximum context length for predictions?\n",
        "max_iters = 5000 # Max iterations we run the optimization\n",
        "# How often we evaluate across the optimization; every 500 iterations\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "\"\"\"\n",
        "Use 'mps' if on a mac as below:\n",
        "\n",
        "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "\"\"\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# How many batches we use each time we evaluate\n",
        "eval_iters = 200\n",
        "d_model = 96\n",
        "n_head = 6 # This implied that each head has a dimension for the key, query, and values of d_model / 6.\n",
        "n_layer = 6 # This implies we have 6 turns to mix the embeddigs; this is \"Nx\" in the paper\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "FILL_IN = \"FILL_IN\"\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "FONrRI-iKuaB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FONrRI-iKuaB",
        "outputId": "db6b912d-471d-423a-e6e7-c4733f3b4cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j\n",
            "To: /content/hemingway.txt\n",
            "\r  0% 0.00/133k [00:00<?, ?B/s]\r100% 133k/133k [00:00<00:00, 3.53MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 'https://drive.google.com/uc?export=download&id=1RlmRmXiWVKpZq98ftdtOIdM2lsA1uw3j'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HXK8qBjo01Yy",
      "metadata": {
        "id": "HXK8qBjo01Yy"
      },
      "source": [
        "As usual, we read the text file and then get two dictionaries from char to idx and in reverse. char embeddings is what we will use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1930b1d3",
      "metadata": {
        "id": "1930b1d3"
      },
      "outputs": [],
      "source": [
        "with open('hemingway.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# Create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # Encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and Test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # First 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bade6f32",
      "metadata": {
        "id": "bade6f32"
      },
      "outputs": [],
      "source": [
        "# Data loading\n",
        "def get_batch(split):\n",
        "    # Generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # Randomly select batch_size rows from data's row indices\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # Select batch_size chuncks of text each of size block_size; stack them\n",
        "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # Do the same for y, but make sure that this is shifted over by 1\n",
        "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # I.e. if you select xb (1, 2, 3, 4), yb should be (2, 3, 4, 5)\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "    # Each of xb, yb should be (batch_size, block_size)\n",
        "    return xb, yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "938086a6",
      "metadata": {
        "id": "938086a6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    # Put the model in eval mode here\n",
        "    model.eval()\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters) # Initilize an array of tensor of zeros of size eval_iters\n",
        "        for k in range(eval_iters):\n",
        "            # Get a batch of data\n",
        "            xb, yb = get_batch(split)\n",
        "            # Get the mean and loss\n",
        "            logits, loss = model(xb, yb)\n",
        "            # Get the loss for this batch\n",
        "            losses[k] = loss.item()\n",
        "        # Insert the mean estimate for the loss, based on the slit you are in\n",
        "        out[split] = losses.mean()\n",
        "    # Put the model in train mode here\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "10e26176",
      "metadata": {
        "id": "10e26176"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    This class represents one head of self-attention\n",
        "    Note that since this is a Decoder, this is masked-self-attention\n",
        "    There is no Encoder, so there is no cross-self-attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_head):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "        # Map each key, query, or value in to a d_head dimensional model.\n",
        "        # Each should be matrices from d_model to d_head\n",
        "        self.W_K = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_Q = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_head, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, T, d_model)\n",
        "        # B = batch_size, T = block_size in the below\n",
        "        B,T,d = x.shape\n",
        "        # Get the key and query representations from the embedding x\n",
        "        # (B,T,d_head)\n",
        "        k = self.W_K(x)\n",
        "        # (B,T,d_head)\n",
        "        q = self.W_Q(x)\n",
        "        # (B,T,d_head)\n",
        "        v = self.W_V(x)\n",
        "\n",
        "        # Compute attention scores, and get the new representations for this head\n",
        "\n",
        "        # (B T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "        # Multiply q by k and divide by the appropriate constant\n",
        "        scores = q @ k.transpose(-2,-1) * self.d_head**-0.5\n",
        "\n",
        "        # (B, T, T)\n",
        "        # Apply a mask to scores, making all scores above the diagonal -inf\n",
        "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # (B, T, T)\n",
        "        # Apply softmax to the final dimension of scores\n",
        "        a =  F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout\n",
        "        a = self.dropout(a)\n",
        "        # Perform the weighted aggregation of the values\n",
        "        # Using a and v, get the new representations\n",
        "        # (B, T, T) @ (B, T, d_head) -> (B, T, d_head)\n",
        "        out = a @ v\n",
        "        # For each token, return the weighted sum of the values\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attention in parallel\n",
        "    You can have just sequential code below\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(d_head) for _ in range(num_heads)])\n",
        "        # This is to project back to the dimension of d_model. In this case, it is just a learned linear map\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the different representations per head along the last dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Project the concatenation and apply dropout; this is the W_O in \"Attention is all you need\"\n",
        "        out = self.dropout(self.W_O(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "XdEtmrPm7ZCD",
      "metadata": {
        "id": "XdEtmrPm7ZCD"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity; this is applied at the token level\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        d_ff = 4 * d_model\n",
        "        # Map each token via a linear map to d_ff, apply ReLU, map back to d_model, and then apply dropout\n",
        "        # This can be done with nn.Sequential\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ddb29049",
      "metadata": {
        "id": "ddb29049"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block: communication followed by computation\n",
        "    These are stacked on top of each other one after another\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super().__init__()\n",
        "        # Each head gets a smaller dimensional representation of the data\n",
        "        # Assume each head gets a representation of dimension d_head and d_model is divisible by n_head\n",
        "        d_head = d_model // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, d_head)\n",
        "        self.ff = FeedFoward(d_model)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This is different from the originl transformer paper\n",
        "        In the \"Attention is all you need\" paper, we had\n",
        "        x = self.ln1(x + self.sa(x))\n",
        "        x = self.ln2(x + self.ffwd(x))\n",
        "        See Figure 1 here, and mimic that: https://arxiv.org/pdf/2002.04745.pdf\n",
        "\n",
        "        Here, you can also do:\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        \"\"\"\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5c230527",
      "metadata": {
        "id": "5c230527"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits for the next token from a lookup table\n",
        "        # Token embeddings are from vocab_size to d_model\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "        # Position embeddings are from block_size (T) to d_model\n",
        "        self.position_embedding_table = nn.Embedding(block_size, d_model)\n",
        "        # This should be n_sequential applications of a DecoderBlock\n",
        "        # This is the \"Nx\" piece in the paper\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(d_model, n_head=n_head) for _ in range(n_layer)])\n",
        "         # Final layer norm\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        # (B,T,d_model)\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        # (T,d_model)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # Add positional encodings to encodings\n",
        "        # (B,T,d_model)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # Mix up the token representations over and over via the blocks\n",
        "        # (B,T,d_model)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Apply layer norm\n",
        "        # (B,T,d_model)\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Apply the final linear map, to get to dimension vocab_size\n",
        "        # (B,T,vocab_size)\n",
        "        logits = self.ff(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, V = logits.shape\n",
        "            logits = logits.view(B*T, V)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        idx is (B, T) array of indices in the current context\n",
        "        This will generate B total paths in parrallel\n",
        "        We will just geenrate 1 batch below\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            # The model only has kowledge of the context of maximum size block_size\n",
        "            # Get the newest (B, T) data; T = block_size\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "\n",
        "            # Get the predictions\n",
        "            # (B, T, vocab_size)\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            # Focus only on the last time step, get the logits\n",
        "            # (B, vocab_size)\n",
        "            logits = logits[:,-1,:]\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            # (B, vocab_size)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution proporttional to probs\n",
        "            # (B, 1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            # (B, T+1)\n",
        "            idx = torch.cat((idx,idx_next), dim=1)\n",
        "        self.train()\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ThiIDDj1gWse",
      "metadata": {
        "id": "ThiIDDj1gWse"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=5, min_delta=0):\n",
        "\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, train_loss, validation_loss):\n",
        "        if (validation_loss - train_loss) / train_loss > self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.tolerance:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sJu3FQkBqT_o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJu3FQkBqT_o",
        "outputId": "e23fd9be-cf52-4a81-cf9b-816f51c9b9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.693182 M parameters\n",
            "step 0: train loss 4.3316, val loss 4.3391\n",
            "step 500: train loss 2.1675, val loss 2.1735\n",
            "step 1000: train loss 1.8632, val loss 1.8599\n",
            "step 1500: train loss 1.6679, val loss 1.6781\n",
            "step 2000: train loss 1.5476, val loss 1.5768\n",
            "step 2500: train loss 1.4668, val loss 1.5087\n",
            "step 3000: train loss 1.4005, val loss 1.4571\n",
            "step 3500: train loss 1.3599, val loss 1.4220\n",
            "step 4000: train loss 1.3164, val loss 1.3947\n",
            "step 4500: train loss 1.2893, val loss 1.3773\n",
            "step 4999: train loss 1.2649, val loss 1.3630\n"
          ]
        }
      ],
      "source": [
        "model = GPT().to(device)\n",
        "# Print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "early_stopping = EarlyStopping(tolerance=1, min_delta=0.2)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        if iter:\n",
        "          scheduler.step()\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        early_stopping(losses['train'], losses['val'])\n",
        "        if early_stopping.early_stop:\n",
        "          print(\"We stop at epoch {}\".format(iter))\n",
        "          break\n",
        "\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2037f214",
      "metadata": {
        "id": "2037f214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec8b314-e594-4f1a-aa0e-5e3f66b94b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Juslife, his who behapped like one the fish and he would be cile ence. But he was chear in the crampelf alade the sard off the swarl ove starter sometived of and he till underves. He said from the with did not reached at the bow from and hand cut beso bone alought.\n",
            "\n",
            "\"And they jumps. But I the turn?\" the eventish of how watching of the ruemes and curring the shoiles. But only are fretwing the roqes of his kight hands were from them with the boy to came talkime and the Tnew throether lines from onlying the old man let plane not sheart in the unarled to great in brother and them must and he him feelt made that the great see and that he could timed when he knew his hurn he has dive beachen were on the shark now cartive handve hurt and the old man he line to dark and the flosh. DiMow might he knew that good he hand was life it cannurracted it he caress. His do in lear to he how quitted that open and he is comme dark. It you and lose them exwith in the water the Water and hoet club get the old man when he had had slank and arong up by the brought. He was from which with when thrurten turne obe the old man the old man looked god. He had rail with into that bestayes deat. He he cliked to fought.\n",
            "\n",
            "\"He sthaps,\" he said and said takes. \"Feed,\" he said said. \"It must must. But a net bearle. Eat fish have up and gooday raiy purpling motthing with a finsubber of and the land restifer his and shad caluts and let of the dicy shangrew in the cold hand. But doo workentifue it left his and gupe spiff-fard in up, as broxt fresh, all a grol put in the like see in I dreaving up bet now.\n",
            "\n",
            "\"I have diket,\" he voy. \"So derrats youbbe, in yirme.\"\n",
            "\n",
            "No almange to firm to the fish I well along the eye ure with thing of the old man. He sleep moven what him than looked the each and bow the brubbed the fish they he tail baits and sarly. EveriMn'ty, a stevily as betting or its fingermsing aloud the oer of the line with flyinde-ying fish, he thought.\n",
            "\n",
            "I have was down upped the gold breat harp on him in the boy and to a goil coillsed in in the boat jirked and sine pralling from line-s resembering.\n",
            "\n",
            "On the old man boy and had restised in to do match. Beso the make thre the sult they readed acr the hard and the he had hand fathed about the fish, surtlener of he water him it rickent in the could had belle and he ot great held wither at the braight.\n",
            "\n",
            "The old man sky and he was good of his fing cleehing he his reved water it the on the would this was nearench. Then he im of for his move,\" he said.\n",
            "\n",
            "He had said too the grear and the sarks nows, fishing paict old man and notts, he watched over, they betwore of hurther he drop'ffers that new of the shoil were the been. It have hand when it the lear bue in the corcle bull ocean a click it.\n",
            "\n",
            "He took it take the mure of the plank drived of the boy line smove in the line into as make. He was not sudderh and you chark unach a time deep, a rest old hand he thought. He line wile sharke over. That behim it the old man was so they fish jrail and him came you have pallued the is after nexced ut uncered. It have betued to lited once him. It do not dight my. You not not to see  vern the old man salt to proped out onto-Uroush the line and letting. There would setching of the night and risping or the borks hander to keeping plow. The strang far anywating his left. The old man shoulde him they water. Then if the bow pill he line his hand toward. I was kiff that he lead and it as neept? He his not with his come, him. I hand fermanything, the had strued mast him but and that love his det now. It had them or it, and gthe fill get and the still of you, he line have, he was steood the liked of the very and not fiver hit. Perk hey had roped or in a man to bad and to smade now evenside now, betted his would some thing undentler and Now pied of the spraps. The skiffely. The plue wide the lookens light kne  keat and lag the water, encing unceantilable. Now he though. Just were soming and the old man made and nothings it. Pencing in the old man, he thought, had sheabbed mined they he fast and had hight startiled That he was should of the fish.\n",
            "\n",
            "\"I'm with mouth hot'ld the dolphin.\"\n",
            "\n",
            "Now is neverything on it once,\" he said, you. \"I must getting. Bet mothy boing good and thught.\"\n",
            "\n",
            "\"That was me reathed the fish,\" he said. \"Whought? I sane, I last beast alme sin off ver. \"Whis have baske and you dry again the fish blark gone is drick and the firsted in the sern and the more may-coll.\"\n",
            "\n",
            "He was sky-fingence, and in the brife and that were as would could afted fastects nour oodser of the boat what see be tast. He oped the hand, from the do who do. I was fish that was a not smalry. Might from If he thought. Coman it never him cloudrly a the water down the seent. Then he could see they aloud far at on or man what it encGed.\n",
            "\n",
            "\"Fing.\"\n",
            "\n",
            "\"He've seing,\" then fish his had should brachings half it brought.\"\n",
            "\n",
            "\"The man't like't too by boy and then sanked he salm the fish. He make ane his head there had began in the shark collina under with with swink with there great in down the old man some and they gunce in the short big working.\n",
            "\n",
            "Shat it his still fish cudder and make left the did noerw the fish, them tait acrosso poined with and dowphin, eater verippe of they You a snark pootded dotreck tread, well jalloht in the to old man deamstar. He shaded not walking.\n",
            "\n",
            "U norsamenting acrosg tod and piess and and might on the fing the go trone to Runder.\"\n",
            "\n",
            "\"I have time you goily juse you but try he sking when the fish head not sparted to a fish in the bow. They just to were preved fast the though. Deed well me time to kill and not Nobetimemble.\n",
            "\n",
            "He put when thre with the fish my, stail that shark against he old man the jing see very and jaws vert it wishore, acrombly. I had skeaff the pasqained to them arm the dark anots and ahots siupers of a dide mouch atite pain and that were besurren the rill rest to they was night. There would man still his dry not surple more what a some that it. He was sterfully. Neep at the boat rove be to looked them came bet baseause out over and scall their soal head, the old man saft hight in he saw them the wailes when I like the can heavy chould \"You'rk way? I must the boy much you frish.\"\n",
            "\n",
            "I\"I risery's have ease you'r eatly suil,\" the old man said. \"Glow.\"\n",
            "\n",
            "\"But pring up eat the old man and chewed hi hight. He cloud water the sun left of the sky now eveny-with.\"\n",
            "\n",
            "He was out his great hand-work planow muchly. \"But it.\n",
            "\n",
            "\"I's know beep the hooked first. But it a right eyes's of the back onlown hours sea and and clike for out the sky bloweding the must.\"\n",
            "\n",
            "\"You and spart the clook,\" he bead. \"I not is the boy he have turns he came too to furtiom and somitten, the great do that fish fish his it. The sun was brothersing the spurpoin in tail water and had about the bow he jerked and the ried him hands as down cut his stee and smorimpbed and held camped and he has defores and the ocean fishing of the roat he had croping, they jumps phoing of the Shoie bunger. The sun gold him brifted coild the old man something to over him noway anotish comparction and compaled nonied turnivelf. Bight too going rom go strivong to jerk whight bess.\n",
            "\n",
            "Then I will it you sonides out the live stire you washed he fruch of he thought. Do that I old race. But e leang?\n",
            "\n",
            "\"The old man's schark clowed when spung it chat himself to beet.\"\n",
            "\n",
            "\"He said that to he cot fish oar and a come,\" the thought. \"What him fast out be toow.\"\n",
            "\n",
            "The boy would all he rom his jumpen the cut on the great for uck across think, the he circle once ut and dreeking sporams any he cutlest against because and the elled reguemb out be the boy blught.\n",
            "\n",
            "\"When It must,\" he said. \"He is haven't was only. Not lone he don't he thought. But what I have was have the fish eat jawsized santed him in the rolicNe of the stard fill it had time in out a monanion. It at kill that him was no line punging now he could nece pullippinc man with sunt arour shome, with the did with in the di; with form anyou anablaserre and that ean it him was it breakines. Fish as they would stervez-st-werectly shark thright of the fish's the live hands as cannot the peoon too the so una never.\n",
            "\n",
            "\"He looked ay rempch's trircle. He still is vo anways mine and painting weight the is raw parkep over you. But some a water you to pectorross of his befortiok and beebait he was back him.\n",
            "\n",
            "The feet his fishermers,\" the old man's boy. \"They boy light likes.\"\n",
            "\n",
            "\"He gept old man clooked and feel.\"\n",
            "\n",
            "\"I'll know frear I can what resh.\"\n",
            "\n",
            "\"Whe old man him with yhat hand a never in the burn-come that for vereenromm on the treathcent light thing any of his chould ramenfich before with his right and the crt. The old man shouldersmeth.\n",
            "\n",
            "Any prything with his packs and bade-warling breenthered on the firsh brought. Nor't he would him and sold see over broat on the could ok at a looked and he thought. He hooked, himself and better and the harpoon and remeth under his jaws and his sterment neglow so you? Not it hand. The old man had vere simes and the old man now dow ever simpting.\n",
            "\n",
            "I was haved the beane i him. When the wanter, steady unsterfors on good on his badly neast evonered and they were in the fyeelteder there that the finlying breeg in the orshears fish and Got from for to ence ocether and it enderes as some. Then two nlece to eat to quick in the felt shaps very as and soadelt im a drathss. Have you meant.\n",
            "\n",
            "It that's slowly down hopphing in the old my man no came lonking. He dopp, he was saw only must. Be I has the knew a there long into the old man was fast but fult too easce but he was was. Cremzemer, so not after he was in the clupped. I love is silredss love, the fish moy and trying fish that and not happed him with his hands and the floot.\n",
            "\n",
            "It raved the flying of anyour like but train. But I flom whould him sleed about coment him and the deaped of it littly the ocen turn,. It ever it wich more in to like out the cub of a parting loose and time bull pide proppenced of only.\n",
            "\n",
            "He dreperen out and to then domphin flying of the make unnew he \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Start the model with a new line, generate up to 10000 tokens\n",
        "# This is technically doing generations in batches, but here we have a batch size of 1 and 1 element to start in the batch\n",
        "# If you have a model that's very large, d_model = 384, n_head = 6, n_layer = 6, you'll get fairly decent results\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "open('fake_hemingway.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8163ae4f",
      "metadata": {
        "id": "8163ae4f"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'gpt.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}